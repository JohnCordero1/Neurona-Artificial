{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "opMSWnbRxTn6",
        "0uBFaod4xmO-",
        "x7-7tfeJ12qm"
      ],
      "authorship_tag": "ABX9TyP3cs8ThhF/w1kCTYpqfLHa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neurona Logística"
      ],
      "metadata": {
        "id": "opMSWnbRxTn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se mencionó, la función de activación **perceptrón** produce una salida binaria {0,1}, basada en que si la entrada supera o no un umbral específico. Esta función escalón hace que no sea diferenciable y, ante la falta de diferenciabilidad y su naturaleza no continua hace que el perceptrón sea dificil de entrenar mediante métodos de optimización.\n",
        "\n",
        "Una solución que habiamos dado a esta falta de diferenciabilidad es tomar la función de activación como una función lineal, talque la salida de la neurona produzca valores reales. Esto nos es muy útil para problemas de regresión, donde el valor de salida toma valores continuos; sin embargo, ¿qué sucede para problemas de clasificación, donde por su naturaleza toma valores discretos? para ello debemos tomar otra función de activación que nos permita transformar la combinación lineal de los valores de entrada y los pesos sinápticos a valores discretos.\n",
        "\n",
        "Por ello, para esta sección se tomará en cuenta la función logística, que pertenence a una familia de funciones denominada función sigmoide. Lla función de activación logística es suave y diferenciable en todo su rango. Esta suavidad y diferenciabilidad son esenciales para el entrenamiento eficiente de redes neuronales utilizando algoritmos de optimización, además que dicha función toma valores de salida entre un rango de [0,1], lo que la hace adecuada para problemas de clasificación binaria.\n",
        "\n",
        "Por lo tanto, podemos escribir nuestro problema de la siguiente manera:\n",
        "\n",
        "\\begin{align}\n",
        "z &= w^T x + b \\\\\n",
        "\\hat{y} &= \\sigma (z) = \\frac{1}{1+ e^{-z}}\n",
        "\\end{align}\n",
        "\n",
        "Con esto logramos cambiar la función de activación. Donde la última expresión es conocida como la función logistica, con $y^{(i)} \\in [0,1]$. Para su correcto uso en problemas de clasificación debemos imponer un umbral."
      ],
      "metadata": {
        "id": "2s1584gVqLby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente descendente en linea"
      ],
      "metadata": {
        "id": "0uBFaod4xmO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que el problema de optimización de la neurona lineal, debemos generar un error con el cual podamos optimizar el problema, para ello se continuará con el uso de la siguiente función de perdida.\n",
        "\n",
        "$$e_1 = L(y,\\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$$\n",
        "\n",
        "Con esto podemos buscar el $argmin$ de la función y expresarla con sus respectivos gradientes, tanto para los pesoss sinápticos como para el sesgo:\n",
        "\n",
        "$$∇_w L(y,\\hat{y}) = - (y - \\hat{y}) \\left[\\hat{y}(1-\\hat{y})\\right]x$$\n",
        "\n",
        "$$\\frac{∂ L(y,\\hat{y}) }{∂ b} = - (y - \\hat{y}) \\left[\\hat{y}(1-\\hat{y})\\right]$$\n",
        "\n",
        "De este modo, podemos llegar a las siguientes ecuaciones para entrenar el algoritmo:\n",
        "\n",
        "\\begin{align}\n",
        "z = w^T x + b \\\\\n",
        "\\hat{y} = \\sigma (z) = \\frac{1}{1+ e^{-z}} \\\\\n",
        "w + η (y - \\hat{y}) \\left[\\hat{y}(1-\\hat{y})\\right] x \\rightarrow w \\\\\n",
        "b + η (y - \\hat{y}) \\left[\\hat{y}(1-\\hat{y})\\right] \\rightarrow b \\\\\n",
        "\\end{align}\n",
        "\n",
        "Con esto $\\hat{y}$ nos dará probabilidades entre $[0,1]$, pero si la pasamos por un umbral nos dará un valor discreto $\\left\\{0,1 \\right\\}$, y con esto resolvemos el problema de clasificación.\n"
      ],
      "metadata": {
        "id": "bjPyGanYx8pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropia Binaria Cruzada"
      ],
      "metadata": {
        "id": "x7-7tfeJ12qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra función de costo que podemos utilzar es la siguiente, conocida como función de entropia cruzada binaria:\n",
        "\n",
        "$$e_3 = L(y,\\hat{y}) = - \\left[y log(\\hat{y}) + (1+y) log(1+ \\hat{y})\\right]$$\n",
        "\n",
        "Si realizamos, la optimización de esta función llegaremos a una grata sorpresa, pues las decisiones de optimización seran las mismas que una función lineal:\n",
        "\n",
        "$$∇_w L(y,\\hat{y}) = - (y - \\hat{y}) x$$\n",
        "\n",
        "$$\\frac{∂ L(y,\\hat{y}) }{∂ b} = - (y - \\hat{y}) $$\n",
        "\n",
        "Por lo tanto, las ecuaciones de actualización quedan:\n",
        "\n",
        "\\begin{align}\n",
        "z = w^T x + b \\\\\n",
        "\\hat{y} = \\sigma (z) = \\frac{1}{1+ e^{-z}} \\\\\n",
        "w + η (y - \\hat{y}) x \\rightarrow w \\\\\n",
        "b + η (y - \\hat{y}) \\rightarrow b \\\\\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "ptAbI5Pw2LQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente descendente por lote"
      ],
      "metadata": {
        "id": "wCu127To6FO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como en el problema de la neurona lineal, también podemos entrenar el modelo por lotes de información, a partir de la siguiene función de costo:\n",
        "\n",
        "$$e_2 = \\frac{1}{2 p} \\sum_{i=1}^p (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
        "\n",
        "Con las siguientes ecuaciones para entrenar el algoritmo:\n",
        "\n",
        "\\begin{align}\n",
        "Z = w^T X + b \\\\\n",
        "\\hat{Y} = \\sigma (Z) = \\frac{1}{1+ e^{-Z}} \\\\\n",
        "w + \\frac{η}{p} \\left[(Y - \\hat{Y}) \\odot \\hat{Y}(1-\\hat{Y})\\right] X^T \\rightarrow w \\\\\n",
        "b + \\frac{η}{p} \\sum_{i=1}^p\\left[\\left(y^{(i)}-\\hat{y}^{(i)}\\right)\\left(\\hat{y}^{(i)}\\left(1-\\hat{y}^{(i)}\\right)\\right)\\right] \\rightarrow b \\\\\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "VJ-Eup8w6mx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programando la neurona logística"
      ],
      "metadata": {
        "id": "EQmPyanb_-lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pTEm9ogmACjl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neurona_logistica:\n",
        "  def __init__(self, n_entrada, factor_aprendizaje=0.1):\n",
        "    self.w = -1 +2 * np.random.rand(n_entrada) # se creara un vector aleatorio entre -1 y 1\n",
        "    self.b = -1 +2 * np.random.rand()\n",
        "    self.eta = factor_aprendizaje\n",
        "\n",
        "  #Creando el método de prediccion, donde solo nos dara la probabilidad\n",
        "  def predict_proba(self, X):\n",
        "    Z = np.dot(self.w, X) + self.b\n",
        "    y_est = 1 / (1 + np.exp(-Z))\n",
        "    return y_est\n",
        "\n",
        "  #Creando el umbral de 0.5\n",
        "  def predict(self, X):\n",
        "    Z = np.dot(self.w, X) + self.b\n",
        "    y_est = 1 / (1 + np.exp(-Z))\n",
        "    return 1* (y_est > 0.5)\n",
        "\n",
        "  #Creando el metodo para entrenar por lotes (entropia cruzada)\n",
        "  def fit(self, X, Y, epochs=100):\n",
        "    p = X.shape[1] #sacamos el indice 1 de X, el número de patrones\n",
        "    for _ in range(epochs):\n",
        "      Y_est = self.predict_proba(X) #para entrenar usamos las probabilidades\n",
        "      #las actualizaciones\n",
        "      self.w += (self.eta/p) * np.dot ((Y-Y_est), X.T).ravel()\n",
        "      self.b += (self.eta/p) *np.sum(Y-Y_est)"
      ],
      "metadata": {
        "id": "cU1SXF9rAJTJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ejemplo\n",
        "X = np.array([[0,0,1,1],\n",
        "              [0,1,0,1]])\n",
        "Y = np.array([[0,0,1,1]])"
      ],
      "metadata": {
        "id": "Bk3nNl6TCtNo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instanciamos\n",
        "neurona = Neurona_logistica(2, 0.9)"
      ],
      "metadata": {
        "id": "pTwcbkbNDElq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtenemos sus probabilidades\n",
        "neurona.predict_proba(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ez9tvj4Ds4y",
        "outputId": "2de77bcb-2195-4a89-a21d-96feaeba3c80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.37635081, 0.54900574, 0.39420163, 0.56759266])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#entrenamos\n",
        "neurona.fit(X,Y)"
      ],
      "metadata": {
        "id": "UlhHtJgbEOI1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observamos si convergio correctamente a sus valores reales de Y\n",
        "neurona.predict_proba(X)\n",
        "#donde las probabilidades estan cerca de los valores reales de Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaoyD38UEp6Y",
        "outputId": "9f6b95b6-cf1d-4940-998b-044b404e5ca4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.07670333, 0.05658144, 0.96181888, 0.94787913])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrc0C98oFCBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
